import numpy as np
import matplotlib.pyplot as plt
import struct
import os
from pathlib import Path
#
# Define the DriverFileReader class that setups the reader for uDALES driver files
#
class DriverFileReader:
    '''
        Read uDALES driver files generated by uDALES simulations
        with domain decomposition in the y-direction.
        Supports reading time stamps and field variables (u, v, w, thl, qt, sv)
        from multiple processors and collating them into full fields.
    
    METHODS:
    -----------
    read_time_file(directory='.')
        Read the time stamp file (tdriver_000.YYY)
    read_field_file_single(field_name, driver_id, ny_local, nz, n_timesteps,
                            directory='.', scalar_fields=1, has_ghost=True)
          Read a single field file from one processor               
    read_field_file(field_name, ny_total, nz, n_timesteps, directory='.',
                    scalar_fields=1, has_ghost=True)
          Read and collate field files from all processors in y-direction
    read_all_fields(ny, nz, directory='.',
                    read_temperature=False, read_moisture=False,
                    read_scalars=False, n_scalars=0, has_ghost=True)
          Read all available driver fields and collate across processors
    -----------
    '''
    
    def __init__(self, experiment_number, nprocy=1, job_number=None):
        '''
            Initialize the driver file reader.
        
        Parameters:
        -----------
        experiment_number : str or int
            3-digit experiment number (e.g., '001', 1)
        nprocy : int
            Number of processors in y-direction (domain decomposition)
        job_number : str or int, optional
            Job number for reading (if different from experiment_number)
        '''
        self.exp_nr = f"{int(experiment_number):03d}"
        self.nprocy = nprocy
        self.job_nr = f"{int(job_number):03d}" if job_number else self.exp_nr
        
    def read_time_file(self, directory='.'):
        '''
            Read the time stamp file (tdriver_000.YYY)
        Parameters:
        -----------
        directory : str
            Directory where the driver files are located
        '''
        # Time file is always from driver_id='000'
        filename = f"tdriver_000.{self.job_nr}"
        filepath = Path(directory) / filename
        
        if not filepath.exists():
            raise FileNotFoundError(f"Time file not found: {filepath}")
        
        record_size = 8  # Double precision float (8 bytes)
        file_size = filepath.stat().st_size
        n_records = file_size // record_size
        
        print(f"Reading {n_records} time stamps from {filename}")
        
        times = np.zeros(n_records)
        
        with open(filepath, 'rb') as f:
            for i in range(n_records):
                data = f.read(record_size)
                if len(data) == record_size:
                    times[i] = struct.unpack('d', data)[0]
        
        return times
    
    def read_field_file_single(self, field_name, driver_id, ny_local, nz, n_timesteps, 
                               directory='.', scalar_fields=1, has_ghost=True):
        '''
            Read a single field file from one processor
        
        Parameters:
        -----------
        driver_id : str
            3-digit driver ID (e.g., '000', '001')
        ny_local : int
            Local number of y grid points for this processor (without ghost cells)
        '''
        filename = f"{field_name}driver_{driver_id}.{self.exp_nr}"
        filepath = Path(directory) / filename
        
        if not filepath.exists():
            raise FileNotFoundError(f"Field file not found: {filepath}")
        
        # Ghost cell configuration
        jh = 1 if has_ghost else 0
        kh = 1 if has_ghost else 0
        ny_local_total = ny_local + 2 * jh
        nz_total = nz + 2 * kh
        
        # Calculate record size
        if field_name == 's':
            record_size = ny_local_total * nz_total * scalar_fields * 8
        else:
            record_size = ny_local_total * nz_total * 8
        
        # Check file size
        file_size = filepath.stat().st_size
        expected_size = record_size * n_timesteps
        
        if abs(file_size - expected_size) > 100:
            # Try to auto-detect dimensions
            actual_vals_per_step = file_size // (n_timesteps * 8)
            if field_name == 's':
                actual_vals_per_step = actual_vals_per_step // scalar_fields
            
            # Factor to find actual dimensions
            for test_nz in range(max(1, nz-50), nz+50):
                if actual_vals_per_step % test_nz == 0:
                    test_ny = actual_vals_per_step // test_nz
                    ny_local_total = test_ny
                    nz_total = test_nz
                    record_size = ny_local_total * nz_total * 8
                    if field_name == 's':
                        record_size *= scalar_fields
                    break
        
        # Allocate array for this processor's data
        if field_name == 's':
            field_data = np.zeros((n_timesteps, ny_local_total, nz_total, scalar_fields))
        else:
            field_data = np.zeros((n_timesteps, ny_local_total, nz_total))
        
        # Read data
        with open(filepath, 'rb') as f:
            for t in range(n_timesteps):
                data = f.read(record_size)
                if len(data) < record_size:
                    print(f"  WARNING: Incomplete record at timestep {t} in {filename}")
                    break
                
                values = np.frombuffer(data, dtype=np.float64)
                
                if field_name == 's':
                    # Fortran order: j varies fastest, then k, then m (Based on moddriver.f90 code -- check for correctness - Akshay Nov 19th 2025)
                    field_data[t] = values.reshape((ny_local_total, nz_total, scalar_fields), order='F')
                else:
                    # Fortran order: j varies fastest, then k
                    field_data[t] = values.reshape((ny_local_total, nz_total), order='F')
        
        return field_data, ny_local_total, nz_total
    
    def read_field_file(self, field_name, ny_total, nz, n_timesteps, directory='.', 
                       scalar_fields=1, has_ghost=True):
        '''
            Read and collate field files from all processors in y-direction
        
        Parameters:
        -----------
        ny_total : int
            Total number of y grid points across all processors (without ghost cells)
        nz : int
            Number of z grid points (without ghost cells)
        '''
        print(f"\nReading field '{field_name}' from {self.nprocy} processor(s)")
        
        # Calculate local ny for each processor
        ny_local = ny_total // self.nprocy
        
        print(f"  Total grid: ny={ny_total}, nz={nz}")
        print(f"  Per processor: ny_local={ny_local}")
        print(f"  Reading from driver_id 000 to {self.nprocy-1:03d}")
        
        # Read data from each processor
        proc_data = []
        for proc_id in range(self.nprocy):
            driver_id = f"{proc_id:03d}"
            print(f"  Reading from driver_id={driver_id}...", end=" ")
            
            data, ny_local_total, nz_total = self.read_field_file_single(
                field_name, driver_id, ny_local, nz, n_timesteps, 
                directory, scalar_fields, has_ghost
            )
            
            proc_data.append(data)
            print(f"Done. Shape: {data.shape}")
        
        
        jh = 1 if has_ghost else 0
                
        print(f"  Collating data along y-axis...")
        
        if len(proc_data) == 1:            
            collated_data = proc_data[0]
        else:            
            if field_name == 's':                
                collated_parts = []
                for i, data in enumerate(proc_data):
                    if i == 0:
                        # First processor: keep all including ghost at end
                        collated_parts.append(data[:, :, :, :])
                    elif i == len(proc_data) - 1:
                        # Last processor: skip ghost at start
                        collated_parts.append(data[:, jh:, :, :])
                    else:
                        # Middle processors: skip ghosts at both ends
                        collated_parts.append(data[:, jh:-jh, :, :])
                
                collated_data = np.concatenate(collated_parts, axis=1)
            else:                
                collated_parts = []
                for i, data in enumerate(proc_data):
                    if i == 0:
                        # First processor: keep all including ghost at end
                        collated_parts.append(data[:, :, :])
                    elif i == len(proc_data) - 1:
                        # Last processor: skip ghost at start
                        collated_parts.append(data[:, jh:, :])
                    else:
                        # Middle processors: skip ghosts at both ends
                        collated_parts.append(data[:, jh:-jh, :])
                
                collated_data = np.concatenate(collated_parts, axis=1)
        
        print(f"  Final collated shape: {collated_data.shape}")
        print(f"  Value range: [{collated_data.min():.6f}, {collated_data.max():.6f}]")
        
        return collated_data
    
    def read_all_fields(self, ny, nz, directory='.', 
                       read_temperature=False, read_moisture=False, 
                       read_scalars=False, n_scalars=0, has_ghost=True):
        '''
            Read all available driver fields and collate across processors
        
        Parameters:
        -----------
        ny : int
            Total number of grid points in y-direction (without ghost cells)
        nz : int
            Number of grid points in z-direction (without ghost cells)
        '''
        # First read time stamps
        times = self.read_time_file(directory)
        n_timesteps = len(times)
        
        data = {
            'times': times,
            'u': self.read_field_file('u', ny, nz, n_timesteps, directory, has_ghost=has_ghost),
            'v': self.read_field_file('v', ny, nz, n_timesteps, directory, has_ghost=has_ghost),
            'w': self.read_field_file('w', ny, nz, n_timesteps, directory, has_ghost=has_ghost)
        }
        
        if read_temperature:
            data['thl'] = self.read_field_file('h', ny, nz, n_timesteps, directory, has_ghost=has_ghost)
            
        if read_moisture:
            data['qt'] = self.read_field_file('q', ny, nz, n_timesteps, directory, has_ghost=has_ghost)
            
        if read_scalars and n_scalars > 0:
            data['sv'] = self.read_field_file('s', ny, nz, n_timesteps, directory, 
                                             scalar_fields=n_scalars, has_ghost=has_ghost)
        
        return data

#
# MAIN FUNCTION
#
if __name__ == "__main__":
    #
    # USER INPUT PARAMETERS
    #
    experiment_number = '001'               # Experiment number
    nprocy = 2                              # Number of processors in y-direction (==procy in namoptions)    
    ny = 512                                # Total number of grid points in y-direction
    nz = 192                                # Total number of grid points in z-direction
    utau = 0.684306811                      # Friction velocity [m/s]
    H = 600.0                               # Domain height [m]
    nu = 1.5e-5                             # Kinematic viscosity [m^2/s]
    Retau = utau*H/nu                       # Friction Reynolds number basedo on domain height 
    data_dir = '.'                          # Directory containing the driver files      
    # Optional
    zfile = 'lscale.inp.002'                # File containing vertical grid spacing information
    ylen = 2500.0                           # Domain length in y-direction [m]
    n_frames = 100                          # Number of frames to save in animation
    save_animation = True                   # Whether to save animation of U over time 
    video_fps = 60                          # FPS for the saved animation
    umax = 18.0                             # Max U velocity for color scale in animation
    calc_rms = False                        # Whether to calculate and plot rms velocity profiles
    #
    # Setup the driver file reader
    reader = DriverFileReader(experiment_number=experiment_number, nprocy=nprocy)            
    try:
        print("="*70)
        print(f"Reading uDALES driver files with nprocy={nprocy}")
        print("="*70)

        data = reader.read_all_fields(
            ny=ny, 
            nz=nz, 
            directory=data_dir,
            read_temperature=False,  
            read_moisture=False,     
            read_scalars=False,      
            n_scalars=0,
            has_ghost=True  
        )
        # Print summary of the driver files read
        print("\n" + "="*70)
        print("Successfully read and collated driver files!")
        print("="*70)
        print(f"Number of time steps: {len(data['times'])}")
        print(f"Time range: {data['times'][0]:.2f} to {data['times'][-1]:.2f} seconds")
        print(f"U velocity shape: {data['u'].shape}")
        print(f"V velocity shape: {data['v'].shape}")
        print(f"W velocity shape: {data['w'].shape}")
        print("="*70)
        # Time and spanwise averaged velocity profiles
        u_mean = np.mean(data['u'], axis=(0, 1))  # Mean over time and y
        v_mean = np.mean(data['v'], axis=(0, 1))
        w_mean = np.mean(data['w'], axis=(0, 1))
        z = np.loadtxt(zfile,skiprows=1)[:,0]
        # Create visualization
        plt.figure(1,figsize=(15,6))
        plt.subplot(2, 2, 1)        
        # Plot 2D field at first timestep
        y = np.linspace(0,ylen,ny+1)
        im0 = plt.pcolormesh(y,z,data['u'][0,1:-1,1:-1].T, cmap='RdBu_r', shading='auto')        
        plt.xlabel('Y index')
        plt.ylabel('Z index')
        plt.title(f'U velocity at t={data["times"][0]:.1f}s')
        plt.axis('equal')
        # Plot 2D field at middle timestep
        mid_t = len(data['times']) // 2
        plt.subplot(2, 2, 3)        
        im1 = plt.pcolormesh(y,z,data['u'][mid_t,1:-1,1:-1].T, cmap='RdBu_r', shading='auto')        
        plt.xlabel('Y index')
        plt.ylabel('Z index')
        plt.title(f'U velocity at t={data["times"][mid_t]:.1f}s')
        plt.axis('equal')
        
        # Plot mean profiles
        plt.subplot(1, 2, 2)        
        plt.semilogx(z*Retau/H,u_mean[1:-1]/utau, 'bo', label='U')
        plt.xlabel('Mean velocity [m/s]')
        plt.ylabel('Z index')
        plt.title('Time and spanwise averaged velocity profiles')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('driver_analysis.png', dpi=150, bbox_inches='tight')
        print("\nSaved visualization to 'driver_analysis.png'")

        #
        # Calculate the rms velocity profiles and the TKE profiles
        #
        if(calc_rms):
            u_rms = np.sqrt(np.mean((data['u'] - u_mean[np.newaxis, np.newaxis, :])**2, axis=(0, 1)))
            v_rms = np.sqrt(np.mean((data['v'] - v_mean[np.newaxis, np.newaxis, :])**2, axis=(0, 1)))
            w_rms = np.sqrt(np.mean((data['w'] - w_mean[np.newaxis, np.newaxis, :])**2, axis=(0, 1)))
            tke = 0.5 * (u_rms**2 + v_rms**2 + w_rms**2)
            plt.figure(2,figsize=(7,6))
            plt.plot(u_rms[1:-1]/utau, z*Retau/H, 'b-', label='u_rms')
            plt.plot(v_rms[1:-1]/utau, z*Retau/H, 'r-', label='v_rms')
            plt.plot(w_rms[1:-1]/utau, z*Retau/H, 'g-', label='w_rms')
            plt.plot(tke[1:-1]/utau**2, z*Retau/H, 'k--', label='TKE')
            plt.ylabel(r'$x_3^+$',fontsize=20)                
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('rms_velocity_profiles.png', dpi=150, bbox_inches='tight')
            print("Saved RMS velocity profiles to 'rms_velocity_profiles.png'")
        #
        # Save animation of U over time (optional)
        #
        if(save_animation):
            import matplotlib.animation as animation
            from tqdm.auto import tqdm 
            fig, ax = plt.subplots(figsize=(15,4))
            def render_frame(t):
                ax.clear()
                pimg = ax.pcolormesh(y, z, data['u'][t,1:-1,1:-1].T, cmap='magma_r', shading='auto')
                ax.set_title(f'U velocity at t={data["times"][t]:.2f}s')
                ax.set_xlabel('Y')
                ax.set_ylabel('Z')
                pimg.set_clim(0, umax)
                ax.set_xlim(0, ylen)
                ax.set_ylim(0, H)

            # Save animation using FFMpegWriter and a tqdm progress bar
            writer = animation.FFMpegWriter(fps=video_fps)
            out_file = 'u_velocity_animation.mp4'
            with writer.saving(fig, out_file, dpi=400):
                #for t in tqdm(range(len(data['times'])), desc='Saving animation', unit='frame'):
                for t in tqdm(range(n_frames), desc='Saving animation', unit='frame'):
                    render_frame(t)
                    fig.canvas.draw()
                    writer.grab_frame()

            print("Saved U velocity animation to 'u_velocity_animation.mp4'")


        
    except FileNotFoundError as e:
        print(f"\nError: {e}")
        print("\nMake sure the driver files exist in the specified directory.")
        print(f"Expected files: tdriver_000.{experiment_number}, udriver_000.{experiment_number}, ...")
        print(f"                through udriver_{nprocy-1:03d}.{experiment_number}, etc.")
    except Exception as e:
        print(f"\nUnexpected error: {e}")
        import traceback
        traceback.print_exc()
